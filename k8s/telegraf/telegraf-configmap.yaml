apiVersion: v1
kind: ConfigMap
metadata:
  name: telegraf-config
  namespace: ibuki-mitarashi-bot
data:
  telegraf.conf: |-
    ########################################
    # Global Agent Configuration
    ########################################
    [agent]
      interval = "10s"
      round_interval = true
      metric_batch_size = 1000
      metric_buffer_limit = 10000
      collection_jitter = "0s"
      flush_interval = "10s"
      flush_jitter = "0s"
      precision = "1ns"

    ########################################
    # OUTPUTS
    ########################################
    [[outputs.influxdb_v2]]
      ## InfluxDB v2 details - configure via env vars in DaemonSet
      ## URL example: https://influxdb.skyia.jp
      urls = ["${INFLUX_URL}"]
      token = "${INFLUX_TOKEN}"
      organization = "${INFLUX_ORG}"
      bucket = "${INFLUX_BUCKET}"

    ########################################
    # INPUTS - system, cpu, mem, disk, processes
    ########################################
    [[inputs.cpu]]
      percpu = true
      totalcpu = true
      collect_cpu_time = false

    [[inputs.mem]]

    [[inputs.disk]]
      ignore_fs = ["tmpfs", "devtmpfs"]

    [[inputs.kernel]]

    [[inputs.processes]]

    [[inputs.net]]

    # kubernetes inputs - requires RBAC to list pods/nodes
    [[inputs.kubernetes]]
      url = ""
      response_timeout = "5s"
      tls_ca = ""

    # docker/container metrics (if node has docker socket)
    [[inputs.docker]]
      endpoint = "unix:///var/run/docker.sock"
      gather_services = false

    ########################################
    # LOGS - tail container logs produced by kubelet
    ########################################
    [[inputs.tail]]
      files = ["/var/log/containers/*.log"]
      from_beginning = false
      name_override = "k8s_container_logs"
      # Kubernetes container logs include a leading timestamp and stream (stdout/stderr)
      # followed by the actual log payload. Use grok to split the line and capture
      # the payload into 'message', then parse it as JSON via processors.parser.
      data_format = "grok"
      grok_patterns = ["%{TIMESTAMP_ISO8601:log_timestamp} %{DATA:stream} %{DATA:flag} %{GREEDYDATA:message}"]
      grok_custom_patterns = ""

    ########################################
    # PROCESSOR/AGGREGATOR examples (optional)
    ########################################
    # If the 'message' field contains JSON, parse it and merge fields into the
    # metric using processors.parser. If your telegraf build does not include
    # the parser processor, remove this block and consider using a telegraf
    # image that includes processors.parser or pre-parsing logs upstream.
    [[processors.parser]]
      parse_fields = ["message"]
      data_format = "json"
      merge = true
      drop_original = false

    [[processors.rename]]
      [[processors.rename.replace]]
        field = "usage_guest"
        dest = "cpu_usage_guest"
